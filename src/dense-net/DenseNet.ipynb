{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-dev20190613\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os, sys, getopt\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import load\n",
    "from densenet import densenet_model\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hyperparameters\n",
    "# data\n",
    "dataset_name = \"rwth\"\n",
    "rotation_range = 10\n",
    "width_shift_range = 0.10\n",
    "height_shift_range = 0.10\n",
    "horizontal_flip = True\n",
    "\n",
    "# model\n",
    "growth_rate = 128\n",
    "nb_layers = [6,12]\n",
    "\n",
    "# training\n",
    "lr = 0.001\n",
    "epochs = 400\n",
    "max_patience = 25\n",
    "\n",
    "# log\n",
    "checkpoints = False\n",
    "log_freq = 1\n",
    "save_freq = 40\n",
    "models_directory = 'models/'\n",
    "results_directory = 'results/'\n",
    "config_directory = 'config/'\n",
    "\n",
    "general_directory = \"/develop/results/\"\n",
    "save_directory = general_directory + \"{}/dense-net/\".format(dataset_name)\n",
    "results = 'epoch,loss,accuracy,test_loss,test_accuracy\\n'\n",
    "\n",
    "date = datetime.now().strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "identifier = \"{}-growth-{}-densenet-{}\".format(\n",
    "    '-'.join([str(i) for i in nb_layers]),\n",
    "    growth_rate, \n",
    "    dataset_name) + date\n",
    "\n",
    "csv_output_map_file = save_directory + dataset_name + \"_densenet.csv\"\n",
    "summary_file = save_directory + 'summary.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = load(dataset_name)\n",
    "\n",
    "n_classes = len(np.unique(y))\n",
    "image_shape = np.shape(x)[1:]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=rotation_range,\n",
    "    width_shift_range=width_shift_range,\n",
    "    height_shift_range=height_shift_range,\n",
    "    horizontal_flip=horizontal_flip,\n",
    "    fill_mode='constant',\n",
    "    cval=0)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0)\n",
    "test_datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = densenet_model(classes=n_classes, shape=image_shape, growth_rate=growth_rate, nb_layers=nb_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(tf.cast(images, tf.float32), training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 3.043458938598633, Train Acc:7.462686538696289, Test Loss: 2.7706422805786133, Test Acc: 8.333333969116211\n",
      "Epoch: 11, Train Loss: 2.2388598918914795, Train Acc:29.6641788482666, Test Loss: 2.798067331314087, Test Acc: 5.909090995788574\n",
      "Epoch: 21, Train Loss: 1.5411089658737183, Train Acc:51.25, Test Loss: 2.986360788345337, Test Acc: 6.25\n",
      "Epoch: 31, Train Loss: 1.1887325048446655, Train Acc:62.05223846435547, Test Loss: 2.3324947357177734, Test Acc: 24.772727966308594\n",
      "Epoch: 41, Train Loss: 0.9962700605392456, Train Acc:68.50746154785156, Test Loss: 1.281662106513977, Test Acc: 60.871212005615234\n",
      "Epoch: 51, Train Loss: 0.9263520836830139, Train Acc:71.1380615234375, Test Loss: 2.57774019241333, Test Acc: 55.30303192138672\n",
      "Epoch: 61, Train Loss: 0.8348059058189392, Train Acc:74.60820770263672, Test Loss: 2.8631341457366943, Test Acc: 57.08333206176758\n",
      "Epoch: 71, Train Loss: 0.77674400806427, Train Acc:77.55596923828125, Test Loss: 2.7536516189575195, Test Acc: 58.219696044921875\n",
      "Epoch: 81, Train Loss: 0.7432730793952942, Train Acc:77.12686920166016, Test Loss: 3.713595151901245, Test Acc: 58.52272415161133\n",
      "Epoch: 91, Train Loss: 0.6669884324073792, Train Acc:79.7947769165039, Test Loss: 4.137704372406006, Test Acc: 60.530303955078125\n",
      "Epoch: 101, Train Loss: 0.6312869787216187, Train Acc:80.9141845703125, Test Loss: 2.8233087062835693, Test Acc: 61.81818389892578\n",
      "Epoch: 111, Train Loss: 0.5927920937538147, Train Acc:81.62313842773438, Test Loss: 2.645193576812744, Test Acc: 65.03787994384766\n",
      "Epoch: 121, Train Loss: 0.6077441573143005, Train Acc:82.85447692871094, Test Loss: 3.649060010910034, Test Acc: 61.81818389892578\n",
      "Epoch: 131, Train Loss: 0.5511748194694519, Train Acc:82.68656921386719, Test Loss: 3.1323177814483643, Test Acc: 61.06060791015625\n",
      "Epoch: 141, Train Loss: 0.5413971543312073, Train Acc:84.30970001220703, Test Loss: 3.576873779296875, Test Acc: 61.51515197753906\n",
      "Epoch: 151, Train Loss: 0.49620553851127625, Train Acc:84.66417694091797, Test Loss: 3.011209726333618, Test Acc: 65.18939208984375\n",
      "Epoch: 161, Train Loss: 0.48715585470199585, Train Acc:86.11940002441406, Test Loss: 2.7098937034606934, Test Acc: 64.16666412353516\n",
      "Epoch: 171, Train Loss: 0.4877992868423462, Train Acc:85.29850769042969, Test Loss: 2.178333282470703, Test Acc: 65.98484802246094\n",
      "Epoch: 181, Train Loss: 0.48603448271751404, Train Acc:85.39179229736328, Test Loss: 3.195610523223877, Test Acc: 65.6439437866211\n",
      "Epoch: 191, Train Loss: 0.4462883770465851, Train Acc:86.67910766601562, Test Loss: 4.035479545593262, Test Acc: 64.92424011230469\n",
      "Epoch: 201, Train Loss: 0.4598570466041565, Train Acc:86.43656921386719, Test Loss: 4.135398864746094, Test Acc: 66.0984878540039\n",
      "Epoch: 211, Train Loss: 0.4444510042667389, Train Acc:87.44403076171875, Test Loss: 2.7785096168518066, Test Acc: 65.79545593261719\n",
      "Epoch: 221, Train Loss: 0.4364069402217865, Train Acc:86.5671615600586, Test Loss: 2.4054667949676514, Test Acc: 62.68939208984375\n",
      "Epoch: 231, Train Loss: 0.4870220720767975, Train Acc:86.60447692871094, Test Loss: 4.254941940307617, Test Acc: 64.31817626953125\n",
      "Epoch: 241, Train Loss: 0.46591833233833313, Train Acc:87.07089233398438, Test Loss: 3.7674214839935303, Test Acc: 64.50757598876953\n",
      "Epoch: 251, Train Loss: 0.42622047662734985, Train Acc:87.8171615600586, Test Loss: 3.6797726154327393, Test Acc: 65.6439437866211\n",
      "Epoch: 261, Train Loss: 0.3974580764770508, Train Acc:89.06716918945312, Test Loss: 3.7114434242248535, Test Acc: 68.21969604492188\n",
      "Epoch: 271, Train Loss: 0.3950110375881195, Train Acc:88.89925384521484, Test Loss: 2.5125341415405273, Test Acc: 65.6439437866211\n",
      "Epoch: 281, Train Loss: 0.3783491551876068, Train Acc:88.75, Test Loss: 3.182687282562256, Test Acc: 68.93939208984375\n",
      "Epoch: 291, Train Loss: 0.3912977874279022, Train Acc:88.41417694091797, Test Loss: 2.4012234210968018, Test Acc: 67.5\n",
      "Epoch: 301, Train Loss: 0.36670172214508057, Train Acc:89.5708999633789, Test Loss: 2.017930507659912, Test Acc: 70.6060562133789\n",
      "Epoch: 311, Train Loss: 0.3372456133365631, Train Acc:90.07463073730469, Test Loss: 2.275834321975708, Test Acc: 70.56817626953125\n",
      "Epoch: 321, Train Loss: 0.34608617424964905, Train Acc:89.94403076171875, Test Loss: 1.827979564666748, Test Acc: 72.15909576416016\n",
      "Epoch: 331, Train Loss: 0.3246522843837738, Train Acc:90.41044616699219, Test Loss: 1.8296246528625488, Test Acc: 71.36363220214844\n",
      "Epoch: 341, Train Loss: 0.3240633010864258, Train Acc:90.63433074951172, Test Loss: 2.0663704872131348, Test Acc: 69.54545593261719\n",
      "Epoch: 351, Train Loss: 0.34007731080055237, Train Acc:90.0186538696289, Test Loss: 2.2709500789642334, Test Acc: 66.28787994384766\n",
      "Epoch: 361, Train Loss: 0.34461885690689087, Train Acc:90.0186538696289, Test Loss: 3.3497347831726074, Test Acc: 67.00757598876953\n",
      "Epoch: 371, Train Loss: 0.3329465389251709, Train Acc:90.85820770263672, Test Loss: 2.8310625553131104, Test Acc: 69.50757598876953\n",
      "Epoch: 381, Train Loss: 0.35561317205429077, Train Acc:90.27985382080078, Test Loss: 38.76520919799805, Test Acc: 61.21212387084961\n",
      "Epoch: 391, Train Loss: 0.3777053952217102, Train Acc:89.53357696533203, Test Loss: 6.192285060882568, Test Acc: 63.93939208984375\n",
      "Epoch: 401, Train Loss: 0.33149734139442444, Train Acc:90.69029998779297, Test Loss: 2.3813562393188477, Test Acc: 70.34090423583984\n",
      "Epoch: 411, Train Loss: 0.28910142183303833, Train Acc:91.64179229736328, Test Loss: 3.0089406967163086, Test Acc: 73.21969604492188\n",
      "Epoch: 421, Train Loss: 0.2679429054260254, Train Acc:92.64925384521484, Test Loss: 4.630129814147949, Test Acc: 72.76515197753906\n",
      "Epoch: 431, Train Loss: 0.3357570767402649, Train Acc:91.47388458251953, Test Loss: 8.731596946716309, Test Acc: 68.67424011230469\n",
      "Epoch: 441, Train Loss: 0.3387935757637024, Train Acc:90.85820770263672, Test Loss: 6.952939033508301, Test Acc: 67.91666412353516\n",
      "Epoch: 451, Train Loss: 0.3161441683769226, Train Acc:92.01492309570312, Test Loss: 15.805639266967773, Test Acc: 72.1212158203125\n",
      "Epoch: 461, Train Loss: 0.3043782114982605, Train Acc:91.64179229736328, Test Loss: 4.10454797744751, Test Acc: 73.67424011230469\n",
      "Epoch: 471, Train Loss: 0.2680668234825134, Train Acc:92.85447692871094, Test Loss: 2.6387815475463867, Test Acc: 75.18939208984375\n",
      "Epoch: 481, Train Loss: 0.39167046546936035, Train Acc:92.10820770263672, Test Loss: 3.7777137756347656, Test Acc: 61.931819915771484\n",
      "Epoch: 491, Train Loss: 0.361930787563324, Train Acc:90.74626922607422, Test Loss: 2.6237637996673584, Test Acc: 62.3863639831543\n",
      "Epoch: 501, Train Loss: 0.30260351300239563, Train Acc:92.276123046875, Test Loss: 1.285810112953186, Test Acc: 75.03787994384766\n",
      "Epoch: 511, Train Loss: 0.23534947633743286, Train Acc:93.48880767822266, Test Loss: 0.8625974059104919, Test Acc: 81.51515197753906\n",
      "Epoch: 521, Train Loss: 0.1717848777770996, Train Acc:94.62686920166016, Test Loss: 0.9787729978561401, Test Acc: 80.79545593261719\n",
      "Epoch: 531, Train Loss: 0.15819041430950165, Train Acc:95.1119384765625, Test Loss: 0.8047301173210144, Test Acc: 82.5\n",
      "Epoch: 541, Train Loss: 0.1540125608444214, Train Acc:95.223876953125, Test Loss: 0.8728383183479309, Test Acc: 80.71969604492188\n",
      "Epoch: 551, Train Loss: 0.11204509437084198, Train Acc:96.66044616699219, Test Loss: 0.875185489654541, Test Acc: 81.4772720336914\n",
      "Epoch: 561, Train Loss: 0.10158791393041611, Train Acc:96.4552230834961, Test Loss: 0.4980334937572479, Test Acc: 86.89393615722656\n",
      "Epoch: 571, Train Loss: 0.11062400043010712, Train Acc:96.32463073730469, Test Loss: 0.4617650806903839, Test Acc: 86.55303192138672\n",
      "Epoch: 581, Train Loss: 0.10455537587404251, Train Acc:96.41790771484375, Test Loss: 0.482968807220459, Test Acc: 87.04545593261719\n",
      "Epoch: 591, Train Loss: 0.0940033495426178, Train Acc:96.60447692871094, Test Loss: 0.394449919462204, Test Acc: 88.14393615722656\n",
      "Epoch: 601, Train Loss: 0.09709029644727707, Train Acc:96.75373077392578, Test Loss: 0.5319514274597168, Test Acc: 85.6439437866211\n",
      "Epoch: 611, Train Loss: 0.08147887885570526, Train Acc:97.12686157226562, Test Loss: 0.3681548237800598, Test Acc: 90.03787994384766\n",
      "Epoch: 621, Train Loss: 0.09252645075321198, Train Acc:96.66044616699219, Test Loss: 0.4781866669654846, Test Acc: 87.42424011230469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 631, Train Loss: 0.10089632123708725, Train Acc:96.71641540527344, Test Loss: 0.5443257689476013, Test Acc: 86.78030395507812\n",
      "Epoch: 641, Train Loss: 0.09585300832986832, Train Acc:96.75373077392578, Test Loss: 0.6266719698905945, Test Acc: 84.6212158203125\n",
      "Epoch: 651, Train Loss: 0.08581169694662094, Train Acc:97.14552307128906, Test Loss: 0.49508875608444214, Test Acc: 87.46212005615234\n",
      "Epoch: 661, Train Loss: 0.09028290957212448, Train Acc:96.95895385742188, Test Loss: 0.3908652365207672, Test Acc: 87.84091186523438\n",
      "Epoch: 671, Train Loss: 0.08635737001895905, Train Acc:96.95895385742188, Test Loss: 0.7959036827087402, Test Acc: 81.74242401123047\n",
      "Epoch: 681, Train Loss: 0.07465259730815887, Train Acc:97.5, Test Loss: 0.5715638399124146, Test Acc: 86.06060791015625\n",
      "Epoch: 691, Train Loss: 0.07253492623567581, Train Acc:97.64925384521484, Test Loss: 0.4582061469554901, Test Acc: 87.42424011230469\n",
      "Epoch: 701, Train Loss: 0.0960066169500351, Train Acc:96.69776153564453, Test Loss: 0.392192006111145, Test Acc: 87.23484802246094\n",
      "Epoch: 711, Train Loss: 0.06433766335248947, Train Acc:97.76119232177734, Test Loss: 0.360382080078125, Test Acc: 88.82575988769531\n",
      "Epoch: 721, Train Loss: 0.08456271141767502, Train Acc:97.20149230957031, Test Loss: 0.6038747429847717, Test Acc: 85.0\n",
      "Epoch: 731, Train Loss: 0.06584558635950089, Train Acc:97.68656158447266, Test Loss: 0.3843074440956116, Test Acc: 89.1287841796875\n",
      "Epoch: 741, Train Loss: 0.07321565598249435, Train Acc:97.5, Test Loss: 0.34919244050979614, Test Acc: 89.96212005615234\n",
      "Epoch: 751, Train Loss: 0.06229470297694206, Train Acc:97.7052230834961, Test Loss: 0.47106894850730896, Test Acc: 88.37120819091797\n",
      "Epoch: 761, Train Loss: 0.07438592612743378, Train Acc:97.35074615478516, Test Loss: 0.4656919240951538, Test Acc: 87.42424011230469\n",
      "Epoch: 771, Train Loss: 0.055454522371292114, Train Acc:98.13432312011719, Test Loss: 0.3869699239730835, Test Acc: 89.01515197753906\n",
      "Epoch: 781, Train Loss: 0.05522412806749344, Train Acc:98.02239227294922, Test Loss: 0.3619854748249054, Test Acc: 90.18939208984375\n",
      "Epoch: 791, Train Loss: 0.049280796200037, Train Acc:98.48880767822266, Test Loss: 0.6234396696090698, Test Acc: 87.5\n"
     ]
    }
   ],
   "source": [
    "# create summary writers\n",
    "train_summary_writer = tf.summary.create_file_writer(save_directory + 'summaries/train/' + identifier)\n",
    "test_summary_writer = tf.summary.create_file_writer(save_directory +  'summaries/test/' + identifier)\n",
    "\n",
    "# create data generators\n",
    "train_gen =  datagen.flow(x_train, y_train, batch_size=16)\n",
    "test_gen = test_datagen.flow(x_test, y_test, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"starting training\")\n",
    "\n",
    "min_loss = 100\n",
    "min_loss_acc = 0\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batches = 0\n",
    "    for images, labels in train_gen:\n",
    "        train_step(images, labels)\n",
    "        batches += 1\n",
    "        if batches >= len(x_train) / 32:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "\n",
    "    batches = 0\n",
    "    for test_images, test_labels in test_gen:\n",
    "        test_step(test_images, test_labels)\n",
    "        batches += 1\n",
    "        if batches >= len(x_test) / 32:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "\n",
    "    if (epoch % log_freq == 0):\n",
    "        results += '{},{},{},{},{}\\n'.format(epoch,\n",
    "                               train_loss.result(),\n",
    "                               train_accuracy.result()*100,\n",
    "                               test_loss.result(),\n",
    "                               test_accuracy.result()*100)\n",
    "        print ('Epoch: {}, Train Loss: {}, Train Acc:{}, Test Loss: {}, Test Acc: {}'.format(epoch,\n",
    "                               train_loss.result(),\n",
    "                               train_accuracy.result()*100,\n",
    "                               test_loss.result(),\n",
    "                               test_accuracy.result()*100))\n",
    "\n",
    "        if (test_loss.result() < min_loss):    \n",
    "            if not os.path.exists(save_directory + models_directory):\n",
    "                os.makedirs(save_directory + models_directory)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(save_directory + models_directory + \"best{}.h5\".format(identifier))\n",
    "            min_loss = test_loss.result()\n",
    "            min_loss_acc = test_accuracy.result()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "            train_loss.reset_states()           \n",
    "            train_accuracy.reset_states()           \n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            test_loss.reset_states()           \n",
    "            test_accuracy.reset_states()   \n",
    "            \n",
    "    if checkpoints and epoch % save_freq == 0:\n",
    "        if not os.path.exists(save_directory + models_directory):\n",
    "            os.makedirs(save_directory + models_directory)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(save_directory + models_directory+\"{}_epoch{}.h5\".format(identifier,epoch))\n",
    "        \n",
    "    if patience >= max_patience:\n",
    "        break\n",
    "\n",
    "if not os.path.exists(save_directory + results_directory):\n",
    "    os.makedirs(save_directory + results_directory)\n",
    "file = open(save_directory + results_directory + 'results-'+ identifier + '.csv','w') \n",
    "file.write(results) \n",
    "file.close()\n",
    "\n",
    "if not os.path.exists(save_directory + config_directory):\n",
    "    os.makedirs(save_directory + config_directory)\n",
    "\n",
    "config = {\n",
    "    'data.dataset_name': dataset_name, \n",
    "    'data.rotation_range': rotation_range, \n",
    "    'data.width_shift_range': width_shift_range, \n",
    "    'data.height_shift_range': height_shift_range, \n",
    "    'data.horizontal_flip': horizontal_flip, \n",
    "    'model.growth_rate': growth_rate, \n",
    "    'model.nb_layers': nb_layers, \n",
    "    'train.lr': lr, \n",
    "    'train.epochs': epochs, \n",
    "    'train.max_patience': max_patience, \n",
    "}\n",
    "\n",
    "with open(save_directory + config_directory + '.json', 'w') as json_file:\n",
    "    json.dump(config, json_file)\n",
    "\n",
    "file = open(summary_file, 'a+') \n",
    "summary = \"{}, {}, dense-net, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\".format(\n",
    "    date, dataset_name, save_directory + config_directory, min_loss, min_loss_acc)\n",
    "file.write(summary)\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
