{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "!pip install -q tf-nightly-gpu-2.0-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-dev20190522\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, ZeroPadding2D, Dense, Dropout, Activation, Convolution2D\n",
    "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras import initializers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import handshape_datasets as hd\n",
    "\n",
    "DATASET_NAME = \"lsa16\"\n",
    "\n",
    "data = hd.load(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = data[0]\n",
    "labels = data[1]['y']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 32, 32, 3), (None,)), types: (tf.float64, tf.uint8)>\n"
     ]
    }
   ],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(536, 32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(Model):\n",
    "    def __init__(self, nb_dense_block=4, growth_rate=32, nb_filter=64, reduction=0.0, \n",
    "                 dropout_rate=0.0, weight_decay=1e-4, classes=1000, batch_size=32):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        self.eps = 1.1e-5\n",
    "                \n",
    "        # compute compression factor\n",
    "        compression = 1.0 - reduction\n",
    "        \n",
    "        self.concat_axis = 3\n",
    "\n",
    "        # From architecture for ImageNet (Table 1 in the paper)\n",
    "        nb_filter = 64\n",
    "        nb_layers = [6,12,24,16] # For DenseNet-121\n",
    "\n",
    "        self.initial_layers = []\n",
    "        self.initial_layers.append(ZeroPadding2D((3, 3), name='conv1_zeropadding', \n",
    "                                   input_shape=(32, 32, 3), batch_size=batch_size))\n",
    "        self.initial_layers.append(Convolution2D(nb_filter, 7, 2, name='conv1', use_bias=False))\n",
    "        self.initial_layers.append(BatchNormalization(epsilon=self.eps, axis=self.concat_axis, name='conv1_bn'))\n",
    "        self.initial_layers.append(Activation('relu', name='relu1'))\n",
    "        self.initial_layers.append(ZeroPadding2D((1, 1), name='pool1_zeropadding'))\n",
    "        self.initial_layers.append(MaxPooling2D((3, 3), strides=(2, 2), name='pool1'))\n",
    "        \n",
    "        self.dense_blocks = []\n",
    "        self.transition_blocks = []\n",
    "\n",
    "        # Add dense blocks\n",
    "        for block_idx in range(nb_dense_block - 1):\n",
    "            stage = block_idx+2\n",
    "            block, nb_filter = self.dense_block(stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "            self.dense_blocks.append(block)\n",
    "            \n",
    "            # Add transition_block\n",
    "            self.transition_blocks.append(self.transition_block(stage, nb_filter, compression=compression, dropout_rate=dropout_rate, weight_decay=weight_decay))\n",
    "            nb_filter = int(nb_filter * compression)\n",
    "\n",
    "        final_stage = stage + 1\n",
    "        block, nb_filter = self.dense_block(final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "        self.dense_blocks.append(block)\n",
    "        \n",
    "        self.final_layers = self.final_block(nb_filter, classes)       \n",
    "    \n",
    "    def final_block(self, nb_filter, classes):\n",
    "        block = []\n",
    "        block.append(BatchNormalization(epsilon=self.eps, axis=self.concat_axis, name='conv_final_blk_bn'))\n",
    "        block.append(Activation('relu', name='relu_final_blk'))\n",
    "        block.append(GlobalAveragePooling2D(name='pool_final'))\n",
    "        block.append(Dense(classes, name='fc6'))\n",
    "        block.append(Activation('softmax', name='prob'))\n",
    "        return block\n",
    "    \n",
    "    def conv_block(self, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
    "        conv_name_base = 'conv' + str(stage) + '_' + str(branch)\n",
    "        relu_name_base = 'relu' + str(stage) + '_' + str(branch)\n",
    "\n",
    "        # 1x1 Convolution (Bottleneck layer)\n",
    "        inter_channel = nb_filter * 4  \n",
    "        block = []\n",
    "        block.append(BatchNormalization(epsilon=self.eps, axis=self.concat_axis, name=conv_name_base+'_x1_bn'))\n",
    "        block.append(Activation('relu', name=relu_name_base+'_x1'))\n",
    "        block.append(Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', use_bias=False))\n",
    "\n",
    "        if dropout_rate:\n",
    "            block.append(Dropout(dropout_rate))\n",
    "\n",
    "        # 3x3 Convolution\n",
    "        block.append(BatchNormalization(epsilon=self.eps, axis=self.concat_axis, name=conv_name_base+'_x2_bn'))\n",
    "        block.append(Activation('relu', name=relu_name_base+'_x2'))\n",
    "        block.append(ZeroPadding2D((1, 1), name=conv_name_base+'_x2_zeropadding'))\n",
    "        block.append(Convolution2D(nb_filter, 3, 1, name=conv_name_base+'_x2', use_bias=False))\n",
    "\n",
    "        if dropout_rate:\n",
    "            block.append(Dropout(dropout_rate))\n",
    "        return block\n",
    "                                        \n",
    "\n",
    "    def dense_block(self, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, \n",
    "                    grow_nb_filters=True):\n",
    "        block = []\n",
    "        for i in range(nb_layers):\n",
    "            branch = i+1\n",
    "            block.append(self.conv_block(stage, branch, growth_rate, dropout_rate, weight_decay))\n",
    "\n",
    "            if grow_nb_filters:\n",
    "                nb_filter += growth_rate\n",
    "\n",
    "        return block, nb_filter\n",
    "                                        \n",
    "    def transition_block(self, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
    "        conv_name_base = 'conv' + str(stage) + '_blk'\n",
    "        relu_name_base = 'relu' + str(stage) + '_blk'\n",
    "        pool_name_base = 'pool' + str(stage) \n",
    "\n",
    "        block = []\n",
    "        block.append(BatchNormalization(epsilon=self.eps, axis=self.concat_axis, name=conv_name_base+'_bn'))\n",
    "        block.append(Activation('relu', name=relu_name_base))\n",
    "        block.append(Convolution2D(int(nb_filter * compression), 1, 1, name=conv_name_base, use_bias=False))\n",
    "\n",
    "        if dropout_rate:\n",
    "            block.append(Dropout(dropout_rate))\n",
    "\n",
    "        block.append(AveragePooling2D((2, 2), strides=(2, 2), name=pool_name_base))\n",
    "\n",
    "        return block\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.initial_layers:\n",
    "            x = layer(x)\n",
    "        i = 0\n",
    "        for transition_block in self.transition_blocks:\n",
    "            concat_feat = x\n",
    "            for conv_block in self.dense_blocks[i]:                                    \n",
    "                for layer in conv_block:                                    \n",
    "                    x = layer(x)\n",
    "                x = tf.concat([concat_feat, x], self.concat_axis)\n",
    "                concat_feat = x\n",
    "            for layer in transition_block:                                    \n",
    "                x = layer(x)\n",
    "            i += 1\n",
    "        concat_feat = x\n",
    "        for conv_block in self.dense_blocks[i]:                                    \n",
    "            for layer in conv_block:                                    \n",
    "                x = layer(x)\n",
    "            x = tf.concat([concat_feat, x], self.concat_axis)\n",
    "            concat_feat = x\n",
    "        for layer in self.final_layers:\n",
    "            x = layer(x)                                      \n",
    "        return x\n",
    "\n",
    "model = DenseNet(classes=16, dropout_rate=0.3)\n",
    "# model.load_weights(weights_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32))\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(tf.cast(images, tf.float32))\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.7782981395721436, Accuracy: 4.664178848266602, Test Loss: 2.776930093765259, Test Accuracy: 3.78787899017334\n",
      "Saved model to disk\n",
      "Epoch 11, Loss: 1.4045051336288452, Accuracy: 50.661460876464844, Test Loss: 1.423429250717163, Test Accuracy: 51.61845779418945\n",
      "Epoch 21, Loss: 0.8005601167678833, Accuracy: 72.08599853515625, Test Loss: 0.8962130546569824, Test Accuracy: 71.24819946289062\n",
      "Epoch 31, Loss: 0.5714297294616699, Accuracy: 80.15767669677734, Test Loss: 0.7267208695411682, Test Accuracy: 77.87145233154297\n",
      "Epoch 41, Loss: 0.44438567757606506, Accuracy: 84.63323974609375, Test Loss: 0.6220327019691467, Test Accuracy: 81.59645080566406\n",
      "Epoch 51, Loss: 0.3576222062110901, Accuracy: 87.63900756835938, Test Loss: 0.5576972365379333, Test Accuracy: 84.36571502685547\n",
      "Epoch 61, Loss: 0.2990003228187561, Accuracy: 89.66539764404297, Test Loss: 0.5287823677062988, Test Accuracy: 86.18355560302734\n",
      "Epoch 71, Loss: 0.25688761472702026, Accuracy: 91.12097930908203, Test Loss: 0.5225052833557129, Test Accuracy: 87.48933410644531\n",
      "Epoch 81, Loss: 0.22517310082912445, Accuracy: 92.21715545654297, Test Loss: 0.5228546857833862, Test Accuracy: 88.47268676757812\n",
      "Epoch 91, Loss: 0.2004287987947464, Accuracy: 93.07241821289062, Test Loss: 0.5253543853759766, Test Accuracy: 89.23992156982422\n",
      "Epoch 101, Loss: 0.1805843710899353, Accuracy: 93.75831604003906, Test Loss: 0.5286933183670044, Test Accuracy: 89.85523223876953\n",
      "Saved model to disk\n",
      "Epoch 111, Loss: 0.16431550681591034, Accuracy: 94.32062530517578, Test Loss: 0.5322137475013733, Test Accuracy: 90.35967254638672\n",
      "Epoch 121, Loss: 0.15073570609092712, Accuracy: 94.78999328613281, Test Loss: 0.535802960395813, Test Accuracy: 90.7807388305664\n",
      "Epoch 131, Loss: 0.1392291635274887, Accuracy: 95.18770599365234, Test Loss: 0.5394718050956726, Test Accuracy: 91.13751983642578\n",
      "Epoch 141, Loss: 0.12935476005077362, Accuracy: 95.52899932861328, Test Loss: 0.5431634187698364, Test Accuracy: 91.44369506835938\n",
      "Epoch 151, Loss: 0.12078821659088135, Accuracy: 95.8250961303711, Test Loss: 0.5468067526817322, Test Accuracy: 91.70931243896484\n",
      "Epoch 161, Loss: 0.1132858470082283, Accuracy: 96.08441162109375, Test Loss: 0.5502496361732483, Test Accuracy: 91.94193267822266\n",
      "Epoch 171, Loss: 0.10666093975305557, Accuracy: 96.31338500976562, Test Loss: 0.5535150170326233, Test Accuracy: 92.14735412597656\n",
      "Epoch 181, Loss: 0.1007680743932724, Accuracy: 96.5170669555664, Test Loss: 0.5565989017486572, Test Accuracy: 92.33007049560547\n",
      "Epoch 191, Loss: 0.09549225866794586, Accuracy: 96.69942474365234, Test Loss: 0.5595949292182922, Test Accuracy: 92.49365234375\n",
      "Epoch 201, Loss: 0.09074139595031738, Accuracy: 96.8636245727539, Test Loss: 0.5625427961349487, Test Accuracy: 92.64096069335938\n",
      "Saved model to disk\n",
      "Epoch 211, Loss: 0.08644085377454758, Accuracy: 97.01227569580078, Test Loss: 0.565363883972168, Test Accuracy: 92.77430725097656\n",
      "Epoch 221, Loss: 0.08252950757741928, Accuracy: 97.1474609375, Test Loss: 0.5681717395782471, Test Accuracy: 92.89558410644531\n",
      "Epoch 231, Loss: 0.07895679771900177, Accuracy: 97.27095031738281, Test Loss: 0.5709291696548462, Test Accuracy: 93.00636291503906\n",
      "Epoch 241, Loss: 0.07568058371543884, Accuracy: 97.38419342041016, Test Loss: 0.5735743641853333, Test Accuracy: 93.10009002685547\n",
      "Epoch 251, Loss: 0.07266542315483093, Accuracy: 97.4884033203125, Test Loss: 0.5761289596557617, Test Accuracy: 93.17880249023438\n",
      "Epoch 261, Loss: 0.06988130509853363, Accuracy: 97.58463287353516, Test Loss: 0.5784043073654175, Test Accuracy: 93.25148010253906\n",
      "Epoch 271, Loss: 0.0673026591539383, Accuracy: 97.67375946044922, Test Loss: 0.5804147124290466, Test Accuracy: 93.31879425048828\n",
      "Epoch 281, Loss: 0.06490755081176758, Accuracy: 97.75654602050781, Test Loss: 0.5822976231575012, Test Accuracy: 93.3813247680664\n",
      "Epoch 291, Loss: 0.0626770481467247, Accuracy: 97.8336410522461, Test Loss: 0.5840308666229248, Test Accuracy: 93.4395523071289\n",
      "Epoch 301, Loss: 0.06059475243091583, Accuracy: 97.90560913085938, Test Loss: 0.5856338143348694, Test Accuracy: 93.49391174316406\n",
      "Saved model to disk\n",
      "Epoch 311, Loss: 0.058646369725465775, Accuracy: 97.97296142578125, Test Loss: 0.5871493816375732, Test Accuracy: 93.5447769165039\n",
      "Epoch 321, Loss: 0.056819379329681396, Accuracy: 98.03610229492188, Test Loss: 0.5886037945747375, Test Accuracy: 93.59246826171875\n",
      "Epoch 331, Loss: 0.05510278046131134, Accuracy: 98.0954360961914, Test Loss: 0.5900216102600098, Test Accuracy: 93.63728332519531\n",
      "Epoch 341, Loss: 0.053486865013837814, Accuracy: 98.15129089355469, Test Loss: 0.5913792848587036, Test Accuracy: 93.6794662475586\n",
      "Epoch 351, Loss: 0.05196302384138107, Accuracy: 98.2039566040039, Test Loss: 0.5926593542098999, Test Accuracy: 93.71924591064453\n",
      "Epoch 361, Loss: 0.06586305797100067, Accuracy: 97.85629272460938, Test Loss: 0.5973041653633118, Test Accuracy: 93.44413757324219\n",
      "Epoch 371, Loss: 0.06794127076864243, Accuracy: 97.78080749511719, Test Loss: 0.5952381491661072, Test Accuracy: 93.32781982421875\n",
      "Epoch 381, Loss: 0.06688866764307022, Accuracy: 97.81407928466797, Test Loss: 0.5965317487716675, Test Accuracy: 93.27825164794922\n",
      "Epoch 391, Loss: 0.06524573266506195, Accuracy: 97.86759948730469, Test Loss: 0.5956884026527405, Test Accuracy: 93.25931549072266\n",
      "Epoch 401, Loss: 0.06361955404281616, Accuracy: 97.9207763671875, Test Loss: 0.5953453779220581, Test Accuracy: 93.23849487304688\n",
      "Saved model to disk\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5003aafa546b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    400\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \"\"\"\n\u001b[1;32m    577\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 578\u001b[0;31m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[1;32m    579\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    580\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    432\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    433\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 434\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    435\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 400\n",
    "    \n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    if (epoch % 10 == 0):\n",
    "        template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "        print (template.format(epoch+1,\n",
    "                               train_loss.result(),\n",
    "                               train_accuracy.result()*100,\n",
    "                               test_loss.result(),\n",
    "                               test_accuracy.result()*100))\n",
    "    \n",
    "    if (epoch % 100 == 0):\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(\"model_epoch{}.h5\".format(epoch))\n",
    "        print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
