{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "!pip install -q tf-nightly-gpu-2.0-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-dev20190531\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, ZeroPadding2D, Dense, Dropout, Activation, Convolution2D, Reshape\n",
    "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras import initializers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import handshape_datasets as hd\n",
    "\n",
    "DATASET_NAME = \"lsa16\"\n",
    "\n",
    "data = hd.load(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = data[0]\n",
    "labels = data[1]['y']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.10,\n",
    "    height_shift_range=0.10,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True)\n",
    "test_datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "#    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "#test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def denseNet_model(growth_rate=32, nb_filter=64, nb_layers = [6,12,24,16], reduction=0.0, \n",
    "             dropout_rate=0.0, weight_decay=1e-4, classes=1000, batch_size=32, with_se_layers=True):\n",
    "    \n",
    "    with_se_layers = with_se_layers\n",
    "\n",
    "    # compute compression factor\n",
    "    compression = 1.0 - reduction\n",
    "\n",
    "    nb_dense_block = len(nb_layers)\n",
    "    # From architecture for ImageNet (Table 1 in the paper)\n",
    "    # nb_filter = 64\n",
    "    # nb_layers = [6,12,24,16] # For DenseNet-121\n",
    "    \n",
    "    img_input = Input(shape=(32, 32, 3), name='data')\n",
    "    \n",
    "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding', \n",
    "                  input_shape=(32, 32, 3), batch_size=batch_size)(img_input)\n",
    "    x = Convolution2D(nb_filter, 7, 2, name='conv1', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='conv1_bn')(x)\n",
    "    x = Activation('relu', name='relu1')(x)\n",
    "    x = ZeroPadding2D((1, 1), name='pool1_zeropadding')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
    "    \n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        stage = block_idx+2\n",
    "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "\n",
    "        if (with_se_layers):\n",
    "            x = se_block(x, stage, 'dense', nb_filter)\n",
    "\n",
    "        # Add transition_block\n",
    "        x = transition_block(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "        nb_filter = int(nb_filter * compression)\n",
    "\n",
    "        if (with_se_layers):\n",
    "            x = se_block(x, stage, 'transition', nb_filter)\n",
    "\n",
    "    final_stage = stage + 1\n",
    "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "\n",
    "    if (with_se_layers):\n",
    "        x = se_block(x, final_stage, 'dense', nb_filter)\n",
    "\n",
    "    x = BatchNormalization(name='conv_final_blk_bn')(x)\n",
    "    x = Activation('relu', name='relu_final_blk')(x)\n",
    "    x = GlobalAveragePooling2D(name='pool_final')(x)\n",
    "    x = Dense(classes, name='fc6')(x)\n",
    "    output = Activation('softmax', name='prob')(x)\n",
    "    \n",
    "    return Model(inputs=img_input, outputs=output)\n",
    "\n",
    "def conv_block(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
    "    conv_name_base = 'conv' + str(stage) + '_' + str(branch)\n",
    "    relu_name_base = 'relu' + str(stage) + '_' + str(branch)\n",
    "\n",
    "    # 1x1 Convolution (Bottleneck layer)\n",
    "    inter_channel = nb_filter * 4  \n",
    "    x = BatchNormalization(name=conv_name_base+'_x1_bn')(x)\n",
    "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
    "    x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', use_bias=False)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # 3x3 Convolution\n",
    "    x = BatchNormalization(name=conv_name_base+'_x2_bn')(x)\n",
    "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
    "    x = ZeroPadding2D((1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
    "    x = Convolution2D(nb_filter, 3, 1, name=conv_name_base+'_x2', use_bias=False)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def se_block(x, stage, previous, nb_filter, ratio = 16):\n",
    "    se_name = 'se' + str(stage) + '_' + previous\n",
    "    init = x\n",
    "    x = GlobalAveragePooling2D(name='global_average_pooling_2d_'+se_name)(x)\n",
    "    x = Dense(nb_filter // ratio, name='dense_relu_'+se_name)(x)\n",
    "    x = Activation('relu', name='relu_'+se_name)(x)\n",
    "    x = Dense(nb_filter, name='dense_sigmoid_'+se_name)(x)\n",
    "    x = Activation('sigmoid', name='sigmoid_'+se_name)(x)\n",
    "    x = tf.expand_dims(x,1)\n",
    "    x = init * tf.expand_dims(x,1) \n",
    "    return x\n",
    "\n",
    "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, \n",
    "                grow_nb_filters=True):\n",
    "    concat_feat = x\n",
    "    for i in range(nb_layers):\n",
    "        branch = i+1\n",
    "        x = conv_block(x, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
    "        concat_feat = tf.concat([concat_feat, x], -1)\n",
    "\n",
    "        if grow_nb_filters:\n",
    "            nb_filter += growth_rate\n",
    "\n",
    "    return concat_feat, nb_filter\n",
    "\n",
    "def transition_block(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
    "    conv_name_base = 'conv' + str(stage) + '_blk'\n",
    "    relu_name_base = 'relu' + str(stage) + '_blk'\n",
    "    pool_name_base = 'pool' + str(stage) \n",
    "\n",
    "    x = BatchNormalization(name=conv_name_base+'_bn')(x)\n",
    "    x = Activation('relu', name=relu_name_base)(x)\n",
    "    x = Convolution2D(int(nb_filter * compression), 1, 1, name=conv_name_base, use_bias=False)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = AveragePooling2D((2, 2), strides=(2, 2), name=pool_name_base)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "model = denseNet_model(classes=16)\n",
    "# model.load_weights(weights_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(tf.cast(images, tf.float32), training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.9487626552581787, Train Acc:9.514925956726074, Test Loss: 2.775405168533325, Test Acc: 3.78787899017334\n",
      "Epoch: 11, Train Loss: 2.2822341918945312, Train Acc:28.731342315673828, Test Loss: 2.852830648422241, Test Acc: 4.318181991577148\n",
      "Epoch: 21, Train Loss: 1.598499059677124, Train Acc:50.14925765991211, Test Loss: 3.0213639736175537, Test Acc: 4.053030014038086\n",
      "Epoch: 31, Train Loss: 1.2896475791931152, Train Acc:59.496273040771484, Test Loss: 2.476020574569702, Test Acc: 20.795454025268555\n",
      "Epoch: 41, Train Loss: 1.0562000274658203, Train Acc:66.92163848876953, Test Loss: 1.4210020303726196, Test Acc: 59.46969985961914\n",
      "Epoch: 51, Train Loss: 0.9621486663818359, Train Acc:70.0186538696289, Test Loss: 2.5954151153564453, Test Acc: 53.44697189331055\n",
      "Epoch: 61, Train Loss: 0.858160674571991, Train Acc:72.9291000366211, Test Loss: 3.1977384090423584, Test Acc: 56.89393997192383\n",
      "Epoch: 71, Train Loss: 0.8164323568344116, Train Acc:74.96268463134766, Test Loss: 3.3099708557128906, Test Acc: 60.45454788208008\n",
      "Epoch: 81, Train Loss: 0.725123941898346, Train Acc:78.00373077392578, Test Loss: 2.3767662048339844, Test Acc: 58.18181610107422\n",
      "Epoch: 91, Train Loss: 0.6772822737693787, Train Acc:79.14179229736328, Test Loss: 2.5393500328063965, Test Acc: 63.10606002807617\n",
      "Epoch: 101, Train Loss: 0.6761857271194458, Train Acc:79.9813461303711, Test Loss: 3.312364339828491, Test Acc: 64.24242401123047\n",
      "Epoch: 111, Train Loss: 0.6426551938056946, Train Acc:80.35447692871094, Test Loss: 3.1156630516052246, Test Acc: 63.44696807861328\n",
      "Epoch: 121, Train Loss: 0.6563667058944702, Train Acc:80.37313079833984, Test Loss: 3.1568448543548584, Test Acc: 59.96212387084961\n",
      "Epoch: 131, Train Loss: 0.5906861424446106, Train Acc:82.08954620361328, Test Loss: 3.254739999771118, Test Acc: 62.65151596069336\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 800\n",
    "models_directory = 'trained-models'\n",
    "results_directory = 'results'\n",
    "results = 'epoch,loss,accuracy,test_loss,test_accuracy\\n'\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "log_freq = 10\n",
    "save_freq = 100\n",
    "identifier = now.strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "train_summary_writer = tf.summary.create_file_writer('summaries/train/'+ identifier)\n",
    "test_summary_writer = tf.summary.create_file_writer('summaries/test/'+ identifier)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    batches = 0\n",
    "    for images, labels in datagen.flow(x_train, y_train, batch_size=32):\n",
    "        train_step(images, labels)\n",
    "        batches += 1\n",
    "        if batches >= len(x_train) / 32:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "\n",
    "    batches = 0\n",
    "    for test_images, test_labels in test_datagen.flow(x_test, y_test, batch_size=32, shuffle=False):\n",
    "        test_step(test_images, test_labels)\n",
    "        batches += 1\n",
    "        if batches >= len(x_test) / 32:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "\n",
    "    if (epoch % log_freq == 0):\n",
    "        results += '{},{},{},{},{}\\n'.format(epoch+1,\n",
    "                               train_loss.result(),\n",
    "                               train_accuracy.result()*100,\n",
    "                               test_loss.result(),\n",
    "                               test_accuracy.result()*100)\n",
    "        \n",
    "        print ('Epoch: {}, Train Loss: {}, Train Acc:{}, Test Loss: {}, Test Acc: {}'.format(epoch+1,\n",
    "                               train_loss.result(),\n",
    "                               train_accuracy.result()*100,\n",
    "                               test_loss.result(),\n",
    "                               test_accuracy.result()*100))\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "            train_loss.reset_states()           \n",
    "            train_accuracy.reset_states()           \n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            test_loss.reset_states()           \n",
    "            test_accuracy.reset_states()           \n",
    "        \n",
    "        \n",
    "    \n",
    "    if (epoch % save_freq == 0):\n",
    "        if not os.path.exists(models_directory):\n",
    "            os.makedirs(models_directory)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(models_directory+\"/dropout_se_model{}_epoch{}.h5\".format(identifier,epoch))\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "file = open(results_directory+'/results'+ identifier + '.csv','w') \n",
    "file.write(results) \n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
