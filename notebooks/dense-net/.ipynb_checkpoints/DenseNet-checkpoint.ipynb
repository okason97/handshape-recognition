{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "!pip install -q tf-nightly-gpu-2.0-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-dev20190522\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, ZeroPadding2D, Dense, Dropout, Activation, Convolution2D, Reshape\n",
    "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras import initializers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import handshape_datasets as hd\n",
    "\n",
    "DATASET_NAME = \"lsa16\"\n",
    "\n",
    "data = hd.load(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = data[0]\n",
    "labels = data[1]['y']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(Model):\n",
    "    def __init__(self, nb_dense_block=4, growth_rate=32, nb_filter=64, reduction=0.0, \n",
    "                 dropout_rate=0.0, weight_decay=1e-4, classes=1000, batch_size=32, with_se_layers=True):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        self.eps = 1.1e-5\n",
    "        \n",
    "        self.with_se_layers = with_se_layers\n",
    "                \n",
    "        # compute compression factor\n",
    "        compression = 1.0 - reduction\n",
    "        \n",
    "        self.concat_axis = 3\n",
    "\n",
    "        # From architecture for ImageNet (Table 1 in the paper)\n",
    "        nb_filter = 64\n",
    "        nb_layers = [6,12,24,16] # For DenseNet-121\n",
    "\n",
    "        self.initial_layers = []\n",
    "        self.initial_layers.append(ZeroPadding2D((3, 3), name='conv1_zeropadding', \n",
    "                                   input_shape=(32, 32, 3), batch_size=batch_size))\n",
    "        self.initial_layers.append(Convolution2D(nb_filter, 7, 2, name='conv1', use_bias=False))\n",
    "        self.initial_layers.append(BatchNormalization(epsilon=self.eps, axis=self.concat_axis, name='conv1_bn'))\n",
    "        self.initial_layers.append(Activation('relu', name='relu1'))\n",
    "        self.initial_layers.append(ZeroPadding2D((1, 1), name='pool1_zeropadding'))\n",
    "        self.initial_layers.append(MaxPooling2D((3, 3), strides=(2, 2), name='pool1'))\n",
    "        \n",
    "        self.dense_blocks = []\n",
    "        self.transition_blocks = []\n",
    "        self.se_dense_blocks = []  \n",
    "        self.se_transition_blocks = []\n",
    "        \n",
    "        # Add dense blocks\n",
    "        for block_idx in range(nb_dense_block - 1):\n",
    "            stage = block_idx+2\n",
    "            block, nb_filter = self.dense_block(stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "            self.dense_blocks.append(block)\n",
    "        \n",
    "            if (with_se_layers):\n",
    "                self.se_dense_blocks.append(self.se_block(stage, 'dense', nb_filter))\n",
    "                        \n",
    "            # Add transition_block\n",
    "            self.transition_blocks.append(self.transition_block(stage, nb_filter, compression=compression, dropout_rate=dropout_rate, weight_decay=weight_decay))\n",
    "            nb_filter = int(nb_filter * compression)\n",
    "            \n",
    "            if (with_se_layers):\n",
    "                self.se_transition_blocks.append(self.se_block(stage, 'transition', nb_filter))\n",
    "\n",
    "        final_stage = stage + 1\n",
    "        block, nb_filter = self.dense_block(final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "        self.dense_blocks.append(block)\n",
    "\n",
    "        if (with_se_layers):\n",
    "            self.se_dense_blocks.append(self.se_block(final_stage, 'dense', nb_filter))\n",
    "        \n",
    "        self.final_layers = self.final_block(nb_filter, classes)       \n",
    "    \n",
    "    def final_block(self, nb_filter, classes):\n",
    "        block = []\n",
    "        block.append(BatchNormalization(epsilon=self.eps, axis=self.concat_axis, name='conv_final_blk_bn'))\n",
    "        block.append(Activation('relu', name='relu_final_blk'))\n",
    "        block.append(GlobalAveragePooling2D(name='pool_final'))\n",
    "        block.append(Dense(classes, name='fc6'))\n",
    "        block.append(Activation('softmax', name='prob'))\n",
    "        return block\n",
    "    \n",
    "    def conv_block(self, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
    "        conv_name_base = 'conv' + str(stage) + '_' + str(branch)\n",
    "        relu_name_base = 'relu' + str(stage) + '_' + str(branch)\n",
    "\n",
    "        # 1x1 Convolution (Bottleneck layer)\n",
    "        inter_channel = nb_filter * 4  \n",
    "        block = []\n",
    "        block.append(BatchNormalization(epsilon=self.eps, axis=self.concat_axis, name=conv_name_base+'_x1_bn'))\n",
    "        block.append(Activation('relu', name=relu_name_base+'_x1'))\n",
    "        block.append(Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', use_bias=False))\n",
    "\n",
    "        if dropout_rate:\n",
    "            block.append(Dropout(dropout_rate))\n",
    "\n",
    "        # 3x3 Convolution\n",
    "        block.append(BatchNormalization(epsilon=self.eps, axis=self.concat_axis, name=conv_name_base+'_x2_bn'))\n",
    "        block.append(Activation('relu', name=relu_name_base+'_x2'))\n",
    "        block.append(ZeroPadding2D((1, 1), name=conv_name_base+'_x2_zeropadding'))\n",
    "        block.append(Convolution2D(nb_filter, 3, 1, name=conv_name_base+'_x2', use_bias=False))\n",
    "\n",
    "        if dropout_rate:\n",
    "            block.append(Dropout(dropout_rate))\n",
    "        return block\n",
    "                                        \n",
    "    def se_block(self, stage, previous, nb_filter, ratio = 16):\n",
    "        se_name = 'se' + str(stage) + '_' + previous\n",
    "        block = []\n",
    "        block.append(GlobalAveragePooling2D(name='global_average_pooling_2d_'+se_name))\n",
    "        block.append(Dense(nb_filter // ratio, name='dense_relu_'+se_name))\n",
    "        block.append(Activation('relu', name='relu_'+se_name))\n",
    "        block.append(Dense(nb_filter, name='dense_sigmoid_'+se_name))\n",
    "        block.append(Activation('sigmoid', name='sigmoid_'+se_name))\n",
    "        return block\n",
    "        \n",
    "\n",
    "    def dense_block(self, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, \n",
    "                    grow_nb_filters=True):\n",
    "        block = []\n",
    "        for i in range(nb_layers):\n",
    "            branch = i+1\n",
    "            block.append(self.conv_block(stage, branch, growth_rate, dropout_rate, weight_decay))\n",
    "\n",
    "            if grow_nb_filters:\n",
    "                nb_filter += growth_rate\n",
    "\n",
    "        return block, nb_filter\n",
    "                                        \n",
    "    def transition_block(self, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
    "        conv_name_base = 'conv' + str(stage) + '_blk'\n",
    "        relu_name_base = 'relu' + str(stage) + '_blk'\n",
    "        pool_name_base = 'pool' + str(stage) \n",
    "\n",
    "        block = []\n",
    "        block.append(BatchNormalization(epsilon=self.eps, axis=self.concat_axis, name=conv_name_base+'_bn'))\n",
    "        block.append(Activation('relu', name=relu_name_base))\n",
    "        block.append(Convolution2D(int(nb_filter * compression), 1, 1, name=conv_name_base, use_bias=False))\n",
    "\n",
    "        if dropout_rate:\n",
    "            block.append(Dropout(dropout_rate))\n",
    "\n",
    "        block.append(AveragePooling2D((2, 2), strides=(2, 2), name=pool_name_base))\n",
    "\n",
    "        return block\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.initial_layers:\n",
    "            x = layer(x)\n",
    "        i = 0\n",
    "        for transition_block in self.transition_blocks:\n",
    "            concat_feat = x\n",
    "            for conv_block in self.dense_blocks[i]:                                    \n",
    "                for layer in conv_block:                                    \n",
    "                    x = layer(x)\n",
    "                x = tf.concat([concat_feat, x], self.concat_axis)\n",
    "                concat_feat = x\n",
    "            if (self.with_se_layers):\n",
    "                init = x\n",
    "                for layer in self.se_dense_blocks[i]:\n",
    "                    x = layer(x)\n",
    "                x = tf.expand_dims(x,1)\n",
    "                x = init * tf.expand_dims(x,1) \n",
    "            for layer in transition_block:                                    \n",
    "                x = layer(x)\n",
    "            if (self.with_se_layers):\n",
    "                init = x\n",
    "                for layer in self.se_transition_blocks[i]:\n",
    "                    x = layer(x)\n",
    "                x = tf.expand_dims(x,1)\n",
    "                x = init * tf.expand_dims(x,1) \n",
    "            i += 1\n",
    "        concat_feat = x\n",
    "        for conv_block in self.dense_blocks[i]:                                    \n",
    "            for layer in conv_block:                                    \n",
    "                x = layer(x)\n",
    "            x = tf.concat([concat_feat, x], self.concat_axis)\n",
    "            concat_feat = x\n",
    "        if (self.with_se_layers):\n",
    "            init = x\n",
    "            for layer in self.se_dense_blocks[i]:\n",
    "                x = layer(x)\n",
    "            x = tf.expand_dims(x,1)\n",
    "            x = init * tf.expand_dims(x,1) \n",
    "        for layer in self.final_layers:\n",
    "            x = layer(x)                                      \n",
    "        return x\n",
    "\n",
    "model = DenseNet(classes=16)\n",
    "# model.load_weights(weights_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32))\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(tf.cast(images, tf.float32))\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 800\n",
    "    \n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    if (epoch % 10 == 0):\n",
    "        template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "        print (template.format(epoch+1,\n",
    "                               train_loss.result(),\n",
    "                               train_accuracy.result()*100,\n",
    "                               test_loss.result(),\n",
    "                               test_accuracy.result()*100))\n",
    "    \n",
    "    if (epoch % 100 == 0):\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(\"dropout_se_model_epoch{}.h5\".format(epoch))\n",
    "        print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
